import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from sklearn.preprocessing import StandardScaler
from collections import Counter

# --- 1. åŠ è½½æ•°æ® ---
print("æ­£åœ¨åŠ è½½æ•°æ®...")
df = pd.read_csv('btc_history_2y.csv')

# --- 2. ç‰¹å¾å·¥ç¨‹ (å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ”¶ç›ŠçŽ‡è€Œéžç»å¯¹ä»·æ ¼) ---
# è®¡ç®—å¯¹æ•°æ”¶ç›ŠçŽ‡ (Log Return)ï¼Œè¿™æ˜¯é‡‘èžå»ºæ¨¡çš„æ ‡å‡†
# å®ƒèƒ½æŠŠéžå¹³ç¨³çš„ä»·æ ¼åºåˆ—å˜æˆå¹³ç¨³åºåˆ—
df['log_ret'] = np.log(df['close'] / df['close'].shift(1))

# æ³¢åŠ¨çŽ‡ç‰¹å¾
df['volatility'] = df['log_ret'].rolling(window=20).std()

# åŠ¨é‡ç‰¹å¾ (RSI)
def get_rsi(series, period=14):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

df['rsi'] = get_rsi(df['close'])

# å‡çº¿åç¦»åº¦ (ä»·æ ¼è·ç¦»å‡çº¿æœ‰å¤šè¿œ)
df['sma_dist'] = (df['close'] - df['close'].rolling(50).mean()) / df['close']

# æ¸…æ´—ç©ºå€¼
df.dropna(inplace=True)

# --- 3. é‡æ–°å®šä¹‰ç›®æ ‡ (Target) ---
# åªæœ‰å½“ä¸‹ä¸€å°æ—¶æ¶¨å¹… > 0.25% (0.0025) æ—¶ï¼Œæ‰æ ‡è®°ä¸º 1 (ä¹°å…¥æœºä¼š)
# è¿™æ · AI å°±ä¸ä¼šè¢«è¿«åŽ»é¢„æµ‹é‚£äº›æ— æ„ä¹‰çš„éœ‡è¡
threshold = 0.0025 
df['future_ret'] = df['close'].shift(-1) / df['close'] - 1
df['Target'] = (df['future_ret'] > threshold).astype(int)

# æ£€æŸ¥ä¸€ä¸‹æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
print(f"æ ·æœ¬åˆ†å¸ƒ: {Counter(df['Target'])}")
# å¦‚æžœ 1 å¤ªå°‘ (æ¯”å¦‚åªæœ‰ 10%)ï¼Œæ¨¡åž‹ä¼šå¾ˆéš¾è®­ç»ƒã€‚ç†æƒ³æƒ…å†µæ˜¯ 1 å æ¯” 30%-40%ã€‚

# --- 4. å‡†å¤‡è¾“å…¥æ•°æ® ---
# æˆ‘ä»¬é€‰å–è¿™å‡ ä¸ªâ€œå¹³ç¨³â€çš„ç‰¹å¾
features = ['log_ret', 'volatility', 'rsi', 'sma_dist']
data = df[features].values
target = df['Target'].values

# æ ‡å‡†åŒ– (StandardScaler æ¯” MinMax æ›´é€‚åˆç”±äºŽæ­£æ€åˆ†å¸ƒçš„æ•°æ®)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# æž„å»ºæ—¶é—´çª—
X = []
y = []
lookback = 48 # ç¼©çŸ­ä¸€ç‚¹ï¼Œçœ‹è¿‡åŽ»48å°æ—¶

for i in range(lookback, len(data_scaled)):
    X.append(data_scaled[i-lookback:i])
    y.append(target[i])

X, y = np.array(X), np.array(y)

# åˆ’åˆ†æ•°æ®é›† (è¿™æ¬¡æˆ‘ä»¬ä¸ä¹±åºï¼Œä¿ç•™æ—¶é—´é¡ºåº)
split = int(len(X) * 0.85) # 85% è®­ç»ƒ
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# è®¡ç®—ç±»åˆ«æƒé‡ (å¦‚æžœæš´æ¶¨çš„æœºä¼šå¾ˆå°‘ï¼Œæˆ‘ä»¬è¦å‘Šè¯‰ AI é‚£ä¸ª 1 å¾ˆçè´µ)
# è¿™èƒ½é˜²æ­¢ AI å·æ‡’å…¨çŒœ 0
total = len(y_train)
pos = np.sum(y_train)
neg = total - pos
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)
class_weight = {0: weight_for_0, 1: weight_for_1}

# --- 5. æ”¹è¿›çš„æ¨¡åž‹ç»“æž„ ---
model = Sequential()
# ç¬¬ä¸€å±‚ï¼šæ›´å¤šç¥žç»å…ƒï¼ŒåŠ  L2 æ­£åˆ™åŒ–æˆ–æ˜¯ BatchNormalization
model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.3)) 

# ç¬¬äºŒå±‚
model.add(LSTM(32, return_sequences=False))
model.add(Dropout(0.3))

model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# ä½¿ç”¨æ›´å°çš„å­¦ä¹ çŽ‡
opt = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

# --- 6. è®­ç»ƒ ---
print("ðŸš€ å¼€å§‹è®­ç»ƒ v2.0 (å¸¦é˜ˆå€¼è¿‡æ»¤)...")
# EarlyStopping: å¦‚æžœè®­ç»ƒä¸å‡†äº†ï¼Œè‡ªåŠ¨æå‰åœæ­¢
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train, 
    epochs=50, 
    batch_size=64, 
    validation_data=(X_test, y_test),
    class_weight=class_weight, # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼Œè§£å†³æ ·æœ¬ä¸å¹³è¡¡
    callbacks=[early_stop],
    verbose=1
)

# --- 7. è¯„ä¼° ---
print("\n" + "="*30)
res = model.evaluate(X_test, y_test)
print(f"å‡†ç¡®çŽ‡ (Accuracy): {res[1]:.2%}")
print(f"æŸ¥å‡†çŽ‡ (Precision - AIè¯´æ¶¨çœŸçš„æ¶¨çš„æ¦‚çŽ‡): {res[2]:.2%}")
print("="*30)

# æ¨¡æ‹Ÿä¿¡å·åˆ†å¸ƒ
preds = model.predict(X_test)
print(f"æµ‹è¯•é›†é¢„æµ‹ä¿¡å·åˆ†å¸ƒ: è¶…è¿‡0.5çš„æ¯”ä¾‹: {np.mean(preds > 0.5):.2%}")