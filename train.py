import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow.keras.models import Sequential # pyright: ignore[reportMissingImports]
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization # pyright: ignore[reportMissingImports]
from sklearn.preprocessing import StandardScaler
from collections import Counter

# --- 1. åŠ è½½æ•°æ® ---
print("æ­£åœ¨åŠ è½½æ•°æ®...")
df = pd.read_csv('btc_history_2y.csv')

# --- 2. ç‰¹å¾å·¥ç¨‹ (å…³é”®ä¿®æ”¹ï¼šä½¿ç”¨æ”¶ç›Šç‡è€Œéç»å¯¹ä»·æ ¼) ---
# è®¡ç®—å¯¹æ•°æ”¶ç›Šç‡ (Log Return)ï¼Œè¿™æ˜¯é‡‘èå»ºæ¨¡çš„æ ‡å‡†
# å®ƒèƒ½æŠŠéå¹³ç¨³çš„ä»·æ ¼åºåˆ—å˜æˆå¹³ç¨³åºåˆ—
df['log_ret'] = np.log(df['close'] / df['close'].shift(1))

# æ³¢åŠ¨ç‡ç‰¹å¾
df['volatility'] = df['log_ret'].rolling(window=20).std()

# åŠ¨é‡ç‰¹å¾ (RSI)
def get_rsi(series, period=14):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

df['rsi'] = get_rsi(df['close'])

# å‡çº¿åç¦»åº¦ (ä»·æ ¼è·ç¦»å‡çº¿æœ‰å¤šè¿œ)
df['sma_dist'] = (df['close'] - df['close'].rolling(50).mean()) / df['close']

# æ¸…æ´—ç©ºå€¼
df.dropna(inplace=True)

# --- 3. é‡æ–°å®šä¹‰ç›®æ ‡ (Target) ---
# åªæœ‰å½“ä¸‹ä¸€å°æ—¶æ¶¨å¹… > 0.25% (0.0025) æ—¶ï¼Œæ‰æ ‡è®°ä¸º 1 (ä¹°å…¥æœºä¼š)
# è¿™æ · AI å°±ä¸ä¼šè¢«è¿«å»é¢„æµ‹é‚£äº›æ— æ„ä¹‰çš„éœ‡è¡
threshold = 0.0025 
df['future_ret'] = df['close'].shift(-1) / df['close'] - 1
df['Target'] = (df['future_ret'] > threshold).astype(int)

# æ£€æŸ¥ä¸€ä¸‹æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹
print(f"æ ·æœ¬åˆ†å¸ƒ: {Counter(df['Target'])}")
# å¦‚æœ 1 å¤ªå°‘ (æ¯”å¦‚åªæœ‰ 10%)ï¼Œæ¨¡å‹ä¼šå¾ˆéš¾è®­ç»ƒã€‚ç†æƒ³æƒ…å†µæ˜¯ 1 å æ¯” 30%-40%ã€‚

# --- 4. å‡†å¤‡è¾“å…¥æ•°æ® ---
# æˆ‘ä»¬é€‰å–è¿™å‡ ä¸ªâ€œå¹³ç¨³â€çš„ç‰¹å¾
features = ['log_ret', 'volatility', 'rsi', 'sma_dist']
data = df[features].values
target = df['Target'].values

# æ ‡å‡†åŒ– (StandardScaler æ¯” MinMax æ›´é€‚åˆç”±äºæ­£æ€åˆ†å¸ƒçš„æ•°æ®)
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data)

# æ„å»ºæ—¶é—´çª—
X = []
y = []
lookback = 48 # ç¼©çŸ­ä¸€ç‚¹ï¼Œçœ‹è¿‡å»48å°æ—¶

for i in range(lookback, len(data_scaled)):
    X.append(data_scaled[i-lookback:i])
    y.append(target[i])

X, y = np.array(X), np.array(y)

# åˆ’åˆ†æ•°æ®é›† (è¿™æ¬¡æˆ‘ä»¬ä¸ä¹±åºï¼Œä¿ç•™æ—¶é—´é¡ºåº)
split = int(len(X) * 0.85) # 85% è®­ç»ƒ
X_train, X_test = X[:split], X[split:]
y_train, y_test = y[:split], y[split:]

# è®¡ç®—ç±»åˆ«æƒé‡ (å¦‚æœæš´æ¶¨çš„æœºä¼šå¾ˆå°‘ï¼Œæˆ‘ä»¬è¦å‘Šè¯‰ AI é‚£ä¸ª 1 å¾ˆçè´µ)
# è¿™èƒ½é˜²æ­¢ AI å·æ‡’å…¨çŒœ 0
total = len(y_train)
pos = np.sum(y_train)
neg = total - pos
weight_for_0 = (1 / neg) * (total / 2.0)
weight_for_1 = (1 / pos) * (total / 2.0)
class_weight = {0: weight_for_0, 1: weight_for_1}

# --- 5. æ”¹è¿›çš„æ¨¡å‹ç»“æ„ ---
model = Sequential()
# ç¬¬ä¸€å±‚ï¼šæ›´å¤šç¥ç»å…ƒï¼ŒåŠ  L2 æ­£åˆ™åŒ–æˆ–æ˜¯ BatchNormalization
model.add(LSTM(64, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.3)) 

# ç¬¬äºŒå±‚
model.add(LSTM(32, return_sequences=False))
model.add(Dropout(0.3))

model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# ä½¿ç”¨æ›´å°çš„å­¦ä¹ ç‡
opt = tf.keras.optimizers.Adam(learning_rate=0.001)
model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])

# --- 6. è®­ç»ƒ ---
print("ğŸš€ å¼€å§‹è®­ç»ƒ v2.0 (å¸¦é˜ˆå€¼è¿‡æ»¤)...")
# EarlyStopping: å¦‚æœè®­ç»ƒä¸å‡†äº†ï¼Œè‡ªåŠ¨æå‰åœæ­¢
early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

history = model.fit(
    X_train, y_train, 
    epochs=50, 
    batch_size=64, 
    validation_data=(X_test, y_test),
    class_weight=class_weight, # è¿™ä¸€æ­¥å¾ˆå…³é”®ï¼Œè§£å†³æ ·æœ¬ä¸å¹³è¡¡
    callbacks=[early_stop],
    verbose=1
)

# --- 7. è¯„ä¼° ---
print("\n" + "="*30)
res = model.evaluate(X_test, y_test)
print(f"å‡†ç¡®ç‡ (Accuracy): {res[1]:.2%}")
print(f"æŸ¥å‡†ç‡ (Precision - AIè¯´æ¶¨çœŸçš„æ¶¨çš„æ¦‚ç‡): {res[2]:.2%}")
print("="*30)

# æ¨¡æ‹Ÿä¿¡å·åˆ†å¸ƒ
preds = model.predict(X_test)
print(f"æµ‹è¯•é›†é¢„æµ‹ä¿¡å·åˆ†å¸ƒ: è¶…è¿‡0.5çš„æ¯”ä¾‹: {np.mean(preds > 0.5):.2%}")

import matplotlib.pyplot as plt
from sklearn.metrics import precision_score, recall_score

# 1. è·å–æ¨¡å‹å¯¹æµ‹è¯•é›†çš„é¢„æµ‹æ¦‚ç‡ (0åˆ°1ä¹‹é—´çš„å°æ•°)
# æ³¨æ„ï¼šè¿™ä¸€æ­¥ä¸éœ€è¦é‡æ–°è®­ç»ƒï¼Œç›´æ¥ç”¨åˆšæ‰è®­ç»ƒå¥½çš„ model
pred_probs = model.predict(X_test, verbose=0)

print(f"\n{'é˜ˆå€¼ (Threshold)':<15} | {'æŸ¥å‡†ç‡ (Precision)':<18} | {'äº¤æ˜“æ¬¡æ•° (Signals)':<15} | {'èƒœç‡ (Win Rate)':<15}")
print("-" * 70)

best_threshold = 0.5
best_precision = 0.0

# 2. å¾ªç¯æµ‹è¯•ä¸åŒçš„é—¨æ§›
for t in [0.50, 0.55, 0.60, 0.65, 0.70, 0.75, 0.80]:
    # å¦‚æœæ¦‚ç‡ > tï¼Œåˆ™æ ‡è®°ä¸º 1 (ä¹°å…¥)ï¼Œå¦åˆ™ä¸º 0
    my_preds = (pred_probs > t).astype(int)
    
    # åªè¦æœ‰è¿‡è‡³å°‘ä¸€æ¬¡ä¹°å…¥ä¿¡å·ï¼Œæ‰è®¡ç®—
    if np.sum(my_preds) > 0:
        prec = precision_score(y_test, my_preds, zero_division=0)
        count = np.sum(my_preds)
        
        # ç®€å•è®¡ç®—ä¸€ä¸‹è¿™éƒ¨åˆ†ä¿¡å·é‡Œçš„å®é™…èƒœç‡ (å’Œ Precision ç±»ä¼¼)
        print(f"{t:<18.2f} | {prec:<22.2%} | {count:<19} | {prec:.2%}")
        
        if prec > best_precision and count > 10: # è‡³å°‘è¦æœ‰10æ¬¡äº¤æ˜“æ‰æœ‰ç»Ÿè®¡æ„ä¹‰
            best_precision = prec
            best_threshold = t
    else:
        print(f"{t:<18.2f} | {'æ— äº¤æ˜“ä¿¡å·':<22} | 0")

print("-" * 70)
print(f"ğŸš€ æœ€ä½³ç­–ç•¥å»ºè®®ï¼šå°†ä¹°å…¥é˜ˆå€¼è®¾å®šä¸º > {best_threshold}")
print(f"é¢„æœŸèƒœç‡å¯æå‡è‡³: {best_precision:.2%}")

# --- å¯è§†åŒ–æ¦‚ç‡åˆ†å¸ƒ ---
# çœ‹çœ‹ AI åˆ°åº•æœ‰å¤šå°‘æ¬¡æ˜¯éå¸¸ç¡®å®šçš„ï¼Ÿ
plt.figure(figsize=(10, 5))
plt.hist(pred_probs, bins=50, alpha=0.75, color='blue', edgecolor='black')
plt.title('AI Prediction Probability Distribution')
plt.xlabel('Probability (0=Bearish, 1=Bullish)')
plt.ylabel('Count')
plt.axvline(x=best_threshold, color='red', linestyle='dashed', linewidth=2, label=f'Best Threshold {best_threshold}')
plt.legend()
plt.show()